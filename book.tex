\documentclass{report}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator{\maxn}{maxnum}
\DeclareMathOperator{\minm}{minnum} \DeclareMathOperator{\maxnum}{MAX\_NUM}
\DeclareMathOperator{\minnum}{MIN\_NUM}
\DeclareMathOperator{\unimass}{UNIVERSE\_MASS}
\DeclareMathOperator{\diff}{diff}
\title{\Huge\sc \textbf{PHILOSOPHI\AE}\\ NATURALIS\\ \textbf{PRINCIPIA}\\ MATHEMATICA}
\author{Al Caveman}
\begin{document}
\maketitle
\tableofcontents

\chapter{What are numbers}
There are 4 number sets, extremely insane $\mathbb{E}$, insane $\mathbb{I}$,
almost sane $\mathbb{A}$ and more sane $\mathbb{M}$. We use insane and less
insane numbers only to define sane and more sane numbers. Then we never use
those insane numbers any more.

In other words, we need insanity to define sanity, but once we have sanity then
we are on roll and can never look back at our insane past (unless we become
insane).

\section{Extremely insane numbers ($\mathbb{E}$)}
$\mathbb{E}$ is essentially what the powerful community of mathematicians names
the set of real numbers $\mathbb{R}$. This community seem to have defined it in
a way that it has no limits in many ways. They call it an infinitely
uncountable set. Totally crazy, yet they call it \emph{real}. We call it
\emph{extremely insane} and use it briefly only to define one number after
which we immediately jump back to our saner reality.

I define that there is a function $\minm : \mathbb{E} \rightarrow
\mathbb{E}$ that for any mass $m \in \mathbb{E}$, it returns the smallest
positive number
ever representable by mass $m$.  Then I define $\minnum = \minm(\unimass)$, which is
the smallest number ever representable by the material universe that is greater
than 0. Self explanatory, no need for excess rigor.

That's it. We don't need $\mathbb{E}$ for anything else (other than
operations that apply on them, like addition, multiplication, etc)

\section{Insane numbers ($\mathbb{I}$)}
$\mathbb{I}$ is essentially what the powerful community of mathematicians names
the set of integers $\mathbb{Z}$. Madness, but I digress, back to mathematics.

I define that there is a function $\maxn : \mathbb{E} \rightarrow
\mathbb{I}$ that for any mass $m \in \mathbb{E}$, it returns the maximum number
ever representable by $m$. Then I define $\maxnum = \maxn(\unimass)$, which is
the largest number ever representable by the material universe.

That's it. Goodbye $\mathbb{I}$.


\section{Almost sane numbers ($\mathbb{A}$)}
Welcome back to almost sanity:
\[
    \mathbb{A} = \{i : i \in \mathbb{I}, -\maxnum \le i \le \maxnum\}
\]
where some impossible becomes possible:
\begin{itemize}
    \item Yes there are minimum and maximum numbers: $\maxnum, -\maxnum$.
    \item Yes the cardinality of $\mathbb{A}$ is known: $|\mathbb{A}| = 1 +
    2\maxnum$.
\end{itemize}


\section{More sane numbers ($\mathbb{M}$)}
Welcome to more sanity:
\[
    \mathbb{M} = \{i : i \in \mathbb{I}, -\maxnum \le i\minnum \le \maxnum\}
\]
where more impossible becomes possible:
\begin{itemize}
    \item Yes infinitesimals exist: $\minnum$ (also known as the universe pixel).
    \item Yes there is a maximum number: $\maxnum$.
    \item Yes there is a minimum number: $\minnum$.
    \item Yes the cardinality of $\mathbb{M}$ is known: $|\mathbb{M}| = 1 +
    2\frac{\maxnum}{\minnum}$.
\end{itemize}



\chapter{Integration}
The goal here is to integrate the area under $f(x)$ from when $x = 0$ until we
reach $x = x_{end}$. What follows here is bunch of steps that shows my thinking
process because \emph{shadowdaemon} asked for it (and also because I am happy now).

\section{Areas of Lots of Extremely Tiny Rectangular Columns}
So to integrate $f(x) = x^2$ we have to keep summing extremely skinny
columns\footnote{\textbf{Note:} \emph{fefelix} of
\emph{freenode/\#gentoo-chat-exile} tried to look smart by attacking my rigor
by saying that the term \emph{skinny columns} is wrong and that it must be
replaced by the term \emph{infinitesimal} (facepalm moment here). He also tried
to look even smarter by using the phrase \emph{In the realm of $\mathbb{R}$}.
Obviously, any half-assed mathematician knows that $\mathbb{R}$ has only a
single infinitesimal which is zero. Yep, zero bitch. So the term
\emph{infinitesimal} is absolutely wrong in this context, thus even worse than
\emph{skinny columns}. The only exception is if \emph{fefelix} wishes to live
in the 1600s.} of, each of width $d$.
\[\begin{split}
  (x+d-x)(x)^2 + \\
  (x+2d-(x+d))(x+d)^2 + \\
  (x+3d-(x+2d))(x+2d)^2 + \\
  (x+4d-(x+3d))(x+3d)^2 + \\
  (x+5d-(x+4d))(x+4d)^2 + \\
  (x+6d-(x+5d))(x+5d)^2 + \\
  %
  (x+7d-(x+6d))(x+6d)^2 + \\
  (x+8d-(x+7d))(x+7d)^2 + \\
  (x+9d-(x+8d))(x+8d)^2 + \\
  (x+10d-(x+9d))(x+9d)^2 + \\
  (x+11d-(x+10d))(x+10d)^2 + \\
  \ldots
\end{split}\]



You see, if $d$ is extremely tiny (near zero), then we will have to sum an
infinite number of those tiny skinny areas. But for simplicity I put $\ldots$
instead.

We can simplify things:
\[\begin{split}
  d(x)^2 + \\
  d(x+d)^2 + \\
  d(x+2d)^2 + \\
  d(x+3d)^2 + \\
  d(x+4d)^2 + \\
  d(x+5d)^2 + \\
  %
  d(x+6d)^2 + \\
  d(x+7d)^2 + \\
  d(x+8d)^2 + \\
  d(x+9d)^2 + \\
  d(x+10d)^2 + \\
  \ldots
\end{split}\]

Let's expand those squares:
\[\begin{split}
  d(x)(x) + \\
  d(x+d)(x+d) + \\
  d(x+2d)(x+2d) + \\
  d(x+3d)(x+3d) + \\
  d(x+4d)(x+4d) + \\
  d(x+5d)(x+5d) + \\
  %
  d(x+6d)(x+6d) + \\
  d(x+7d)(x+7d) + \\
  d(x+8d)(x+8d) + \\
  d(x+9d)(x+9d) + \\
  d(x+10d)(x+10d) + \\
  \ldots
\end{split}\]

Let's multiply them square bitches:
\[\begin{split}
  dx^2 + \\
  d(x^2 + 2dx + d^2) + \\
  d(x^2 + 4dx + 4d^2) + \\
  d(x^2 + 6dx + 9d^2) + \\
  d(x^2 + 8dx + 16d^2) + \\
  d(x^2 + 10dx + 25x^2) + \\
  %
  d(x^2+12dx   +36d^2) + \\
  d(x^2+14dx   +49d^2) + \\
  d(x^2+16dx   +64d^2) + \\
  d(x^2+18dx   +81d^2) + \\
  d(x^2+20dx   +100d^2) + \\
  \ldots
\end{split}\]


Let's now multiply those bitches with $d$ so that the shit gets spread even
more:
\[\begin{split}
  dx^2 + \\
  dx^2 + 2d^2x + d^3 + \\
  dx^2 + 4d^2x + 4d^3 + \\
  dx^2 + 6d^2x + 9d^3 + \\
  dx^2 + 8d^2x + 16d^3 + \\
  dx^2 + 10d^2x + 25x^3 + \\
  %
  dx^2+12d^2x   +36d^3 + \\
  dx^2+14d^2x   +49d^3 + \\
  dx^2+16d^2x   +64d^3 + \\
  dx^2+18d^2x   +81d^3 + \\
  dx^2+20d^2x   +100d^3 + \\
  \ldots
\end{split}\]

\section{Approximating the Area}
Now this is a critical point. Below is basically saying that each row is an
approximation for the area under $f(x)$ from $x = 0$ till $x = x_{end}$. So the
$1^{st}$ one is a shit approximation where were approximate the area under that
curve by only one big fat column; so $d$ is so huge here, in fact $d =
x_{end}$.

Then, the 2nd line show a slightly less shit approximation where we approximate
the area under the curve by two fat ass rectangular columns. So here $d =
\frac{x_{end}}{2}$.

So the approximation of the area under the curve gets more and more accurate in
each line.
\[\begin{split}
  dx^2\\
  2dx^2 + 2d^2x + d^3\\
  3dx^2 + 6d^2x + 5d^3\\
  4dx^2 + 12d^2x + 14d^3\\
  5dx^2 + 20d^2x + 30d^3\\
  6dx^2 + 30d^2x + 55d^3\\
  7dx^2 + 42d^2x + 91d^3\\
  8dx^2 + 56d^2x + 140d^3\\
  9dx^2 + 72d^2x + 204d^3\\
  10dx^2 + 90d^2x + 285d^3\\
  11dx^2 + 110d^2x + 385d^3\\
  %dx^2 + 2d^2x + d^3\\
  %dx^2 + 4d^2x + 4d^3\\
  %dx^2 + 6d^2x + 9d^3\\
  %dx^2 + 8d^2x + 16d^3\\
  %dx^2 + 10d^2x + 25x^3\\
  %
  %dx^2+12d^2x   +36d^3 + \\
  %dx^2+14d^2x   +49d^3 + \\
  %dx^2+16d^2x   +64d^3 + \\
  %dx^2+18d^2x   +81d^3 + \\
  %dx^2+20d^2x   +100d^3 + \\
\end{split}\]

So basically, you see there is a pattern. The coefficient of the $1^{st}$ term
is easy peasy (just incrementing from 1 to $\infty$). The coefficient from the
$2^{nd}$ term is kinda interesting, it follows the equation $(i^2+i)$ where $i$
is the line number. Note that we start counting lines from 0. So the $1^{st}$
has $i = 0$ and the $2^{nd}$ line has $i = 1$, etc. Finally, the last term is
kinda cool, it follows the pattern $\frac{(i^2+i)(2i+1)}{6}$.

Now you may ask, how did I find these patterns? Well these are well known
number series. You can look them up in the On-Line Encyclopedia of Integer
Sequences\footnote{http://oeis.org/}.

So, the area under the curve of $f(x)$ from $x = 0$ up to $x_{end}$, by any
$d$ (and its corresponding $i$) is:
\[\begin{split}
  dx^2  +  (i^2+i)d^2x   +  \frac{(i^2+i)(2i+1)}{6}d^3\\
\end{split}\]

Now we are almost done. We know that $x = 0$, so we can cancel a few terms:
\begin{equation}\label{eq:dick}\begin{split}
  d0^2  +  (i^2+i)d^20   +  \frac{(i^2+i)(2i+1)}{6}d^3\\
  \frac{(i^2+i)(2i+1)}{6}d^3\\
\end{split}\end{equation}

Of course, we could've canceled those terms that multiply against zero earlier,
but I didn't for random reasons. I just didn't. That's the randomness of life.
But it's all mathematically correct as my caveman balls tell.

You can code a simple script that you give it $x_{end}$ and $d$, by which it
automatically finds $i = x_{end}/d$. You will notice that as $d$ gets smaller,
you end up approaching some limit after which reduction in $d$ does not cause
any change in the estimated area under the curve.

\section{The Precise Area Under The Bitch}
Now let's find the ultimate precision in the limit as $i \rightarrow
\infty$ which also means that $d \rightarrow 0$. But how about not?
Cause it's too hard to solve the limit when two variables are
approaching different limits.

To simplify the limits in an easier way, let's represent $i$ in terms of $d$
and $x_{end}$ as follows $i = x_{end}/d$. Then the same equation would become
as follows:
\[\begin{split}
  \frac{((x_{end}/d)^2+(x_{end}/d))(2(x_{end}/d)+1)}{6}d^3\\
  \frac{(\frac{x_{end}^2}{d^2}+\frac{x_{end}}{d})(\frac{2x_{end}}{d}+1)}{6}d^3\\
  \frac{(\frac{x_{end}^2}{d^2}+\frac{x_{end}}{d})(\frac{2x_{end}}{d}+1)}{6}d^3\\
  \frac{\frac{1}{d}(\frac{x_{end}^2}{d}+x_{end})(\frac{2x_{end}}{d}+1)}{6}d^3\\
  \frac{(\frac{x_{end}^2}{d}+x_{end})(\frac{2x_{end}}{d}+1)}{6}d^2\\
  \frac{\frac{2x_{end}^3}{d^2} +  \frac{x_{end}^2}{d} +  \frac{2x_{end}^2}{d} + x_{end}}{6}d^2\\
  \frac{\frac{2x_{end}^3d^2}{d^2} +  \frac{x_{end}^2d^2}{d} +  \frac{2x_{end}^2d^2}{d} + x_{end}d^2}{6}\\
  \frac{2x_{end}^3 +  x_{end}^2d +  2x_{end}^2d + x_{end}d^2}{6}\\
\end{split}\]

Now, it's super easy. We have to find the limit of that equation as a single
variable approaches 0 (we got rid of $i$). The equation becomes:
\[\begin{split}
  \frac{2x_{end}^3}{6}\\
  \frac{x_{end}^3}{3}\\
\end{split}\]

That's it. Integration re-invented bitch :) --- $\frac{x_{end}^2}3$.

Q.E. freaking DEE.


    \section{Generalizing That}
    Let's integrate $x^c$ where $x, c \in \mathbb{R}$. How easy is that? Let's
    try.

    So, back to the business of summing skinny columns:
    \[\begin{split}
        d \times d^c + \\
        d \times (2d)^c + \\
        d \times (3d)^c + \\
        d \times (4d)^c + \\
        d \times (5d)^c + \\
        \ldots
    \end{split}\]

    Simplified to:
    \[\begin{split}
        d \times d^c + \\
        d \times 2^c \times d^c + \\
        d \times 3^c \times d^c + \\
        d \times 4^c \times d^c + \\
        d \times 5^c \times d^c + \\
        \ldots
    \end{split}\]

    As we try to estimate that sum better we get:
    \[\begin{split}
        nd \times (\frac{n(n+1)}{2})^c \times d^c
    \end{split}\]




\chapter{Differentiation}
Here we want to find the slope of $f(x)$ at point $x$. This is easy so I won't
say much here.

Differentiate $x^2$.

$\frac{f(x+d) - f(x)}{x + d - x}$

$\frac{(x+d)^2 - x^2}{x + d - x}$

$\frac{(x+d)^2 - x^2}{d}$

$\frac{(x+d)(x+d) - x^2}{d}$

$\frac{x^2 + xd + xd + d^2 - x^2}{d}$

$\frac{x^2}{d} + \frac{2xd}{d} + \frac{d^2}{d} - \frac{x^2}{d}$

$\frac{x^2}{d} + 2x + d - \frac{x^2}{d}$

$2x + d$

Now, as $d \rightarrow 0$, it becomes $2x$. Done.

\section{Theorems}
My symbol for differentiation is $\diff$. That's it. Now, if $t, x, c$ are any
numbers, except $c$ is just a non-zero natural number, then:
\begin{theorem}
    $\diff tx^c = ctx^{c-1}$.
\end{theorem}


Okay let's do this... let's assume that $f(x) = tx^c$ for a wise reason that I
need not explain. Just trust me that I am doing the right thing.
\begin{proof}
    \[\begin{split}
        \diff f(x) &= \frac{f(x+d) - f(x)}{(x+d)-x}\\
                   &= \frac{f(x+d) - f(x)}{d}\\
                   &= \frac{t(x+d)^c - tx^c}{d}\\
                   &= \frac{t(\sum_{n=0}^c {c \choose c-n}x^{c-n}d^n) - tx^c}{d}\\
                   &= \frac{(\sum_{n=0}^c t {c \choose c-n}x^{c-n}d^n) - tx^c}{d}\\
                   &= \frac{t {c \choose c-0}x^{c-0}d^0 + (\sum_{n=1}^c t {c \choose c-n}x^{c-n}d^n) - tx^c}{d}\\
                   &= \frac{tx^{c} + (\sum_{n=1}^c t {c \choose c-n}x^{c-n}d^n) - tx^c}{d}\\
                   &= \frac{\sum_{n=1}^c t {c \choose c-n}x^{c-n}d^n}{d}\\
                   &= \sum_{n=1}^c \frac{t {c \choose c-n}x^{c-n}d^n}{d}\\
                   &= \sum_{n=1}^c t {c \choose c-n}x^{c-n}d^{n-1}\\
                   &= t {c \choose c-1}x^{c-1}d^{1-1} + \sum_{n=2}^c t {c \choose c-n}x^{c-n}d^{n-1}\\
                   &= tcx^{c-1} + \sum_{n=2}^c t {c \choose c-n}x^{c-n}d^{n-1}\\
    \end{split}\]

    Then:
    \[\lim_{d \rightarrow 0} tcx^{c-1} + \sum_{n=2}^c t {c \choose
    c-n}x^{c-n}d^{n-1} = tcx^{c-1}\]

    OMG it's the Q.E.D. baby!
\end{proof}


\chapter{Multivariate Stuff}
    \section{Functions that are sum of other linear functions}
        Let's say that we got linear functions $b(n)$ and $c(m)$. By
        definition, this means that:
        \begin{equation}
            b(n) = c_1 + w_1n
        \end{equation}
        \begin{equation}
            c(m) = c_2 + w_2m
        \end{equation}

        \begin{theorem}
            If $a(n, m) = b(n) + c(m)$, then $\lim_{(n,m) \rightarrow (x,y)} a(n,
            m) = \lim_{n \rightarrow x} b(n) + \lim_{m \rightarrow y} c(m)$
        \end{theorem}

        \begin{proof}
            Limits of $b$ and $c$:
            \begin{equation}
                \begin{split}
                    \lim_{n \rightarrow x} b(n) =& \lim_{n \rightarrow x} c_1 +
                    w_1n\\
                    =& c_1 + w_1x\\
                \end{split}
            \end{equation}
            \begin{equation}
                \begin{split}
                    \lim_{m \rightarrow y} c(m) =& \lim_{n \rightarrow y} c_2 +
                    w_2m\\
                    =& c_2 + w_2y\\
                \end{split}
            \end{equation}

            Limit of $a$ by theorem:
            \begin{equation}
                \begin{split}
                    \lim_{(n,m) \rightarrow (x,y)} a(n, m) &= c_1 + w_1x + c_2 + w_2y\\
                \end{split}
            \end{equation}
            
            By substitution we get expanded $a$:
            \begin{equation}
                a(n, m) = c_1 + w_1n + c_2 + w_2m
            \end{equation}

            Limit of expanded $a$:
            \begin{equation}
                \begin{split}
                    \lim_{(n,m) \rightarrow (x,y)} a(n, m) &= \lim_{(n,m)
                    \rightarrow (x,y)} c_1 + w_1n + c_2 + w_2m\\
                    &= c_1 + w_1x + c_2 + w_2y\\
                \end{split}
            \end{equation}

            Guess wat, limit of expanded $a$ is the same as the original one.
            Who could've thought?
        \end{proof}

    \section{Less Gay Stuff}
        Let's say that we wish to differentiate $f(x,y) = x^2/y$. What does it mean? The slope exactly at point $(x,y)$ that touches $f$? And by the way, my notation for that differentiation is: $\diff^f_{(x,y)} f(x,y)$ which means differentiate $f$ with respect to $(x,y)$.
        
        Obviously, there are infinitely many slopes cause we are dealing with surfaces (not curves). So in my logic, you 
\end{document}
